{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to detect objects from the surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from Model import Model\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "from utils import find_central_point, surf\n",
    "from utils import find_corner, returnHeatmap\n",
    "classes = np.array([\n",
    "        'aeroplane',\n",
    "        'bicycle',\n",
    "        'bird',\n",
    "        'boat',\n",
    "        'bottle',\n",
    "        'bus',\n",
    "        'car',\n",
    "        'cat',\n",
    "        'chair',\n",
    "        'cow',\n",
    "        'diningtable',\n",
    "        'dog',\n",
    "        'horse',\n",
    "        'motorbike',\n",
    "        'person',\n",
    "        'pottedplant',\n",
    "        'sheep',\n",
    "        'sofa',\n",
    "        'train',\n",
    "        'tvmonitor',\n",
    "    ])\n",
    "image_path = '../VOCdevkit/VOC2007/JPEGImages/008228.jpg'\n",
    "annotation_path = '../VOCdevkit/VOC2007/Annotations/008228.xml'\n",
    "#image_path = '/media/harvey/FP1/voc/chair/000186.jpg'\n",
    "#annotation_path = 'data/VOCdevkit/VOC2007/Annotations/000186.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rec(filename):\n",
    "    #extract annotation form data\n",
    "    tree = ET.parse(filename)\n",
    "    objects = []\n",
    "    for obj in tree.findall('object'):\n",
    "        obj_struct = {}\n",
    "        obj_struct['name'] = obj.find('name').text\n",
    "        obj_struct['pose'] = obj.find('pose').text\n",
    "        obj_struct['truncated'] = int(obj.find('truncated').text)\n",
    "        obj_struct['difficult'] = int(obj.find('difficult').text)\n",
    "        bbox = obj.find('bndbox')\n",
    "        obj_struct['bbox'] = [int(bbox.find('xmin').text),\n",
    "                              int(bbox.find('ymin').text),\n",
    "                              int(bbox.find('xmax').text),\n",
    "                              int(bbox.find('ymax').text)]\n",
    "        objects.append(obj_struct)\n",
    "        \n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectdetection(input, threshod=130):\n",
    "    #object detection\n",
    "    net = Model('densenet')\n",
    "    net.load_state_dict(torch.load('weights/epoch204densenet.pth'))\n",
    "    net.eval()\n",
    "    \n",
    "    features_blosbs = []\n",
    "    def hook_feature(module, input, output):\n",
    "        #extract feature from the last layer\n",
    "        features_blosbs.append(output.data.cpu().numpy())\n",
    "        \n",
    "    net._modules.get('features').register_forward_hook(hook_feature)\n",
    "    \n",
    "    params = list(net.parameters())\n",
    "    weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "    \n",
    "    #input preprocessing\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225])\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    #load image\n",
    "    img_pil = Image.open(input)\n",
    "\n",
    "    img_tensor = preprocess(img_pil)\n",
    "    img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "    logit = net(img_variable)\n",
    "    \n",
    "    h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    probs = probs.numpy()\n",
    "    idx = idx.numpy()\n",
    "    #generate bounding box\n",
    "    img = cv2.imread(input)\n",
    "    height, width, _ = img.shape  \n",
    "    print('image_shape', img.shape)\n",
    "    \n",
    "\n",
    "    print('{:.3f} -> {}'.format(probs[0], classes[idx[0]]))\n",
    "    print('index of class', idx[0])\n",
    "    CAMs = returnCAM(features_blosbs[0], weight_softmax, [idx[0]])\n",
    "    print('size of CAMs', len(CAMs), len(CAMs[0]), len(CAMs[0]))\n",
    "    resized_heat_map = cv2.resize(CAMs[0],(width, height)) \n",
    "    accumulated_heat_map, maxi_loc = accumulate_around_pixels(resized_heat_map)\n",
    "    max_x, max_y = maxi_loc\n",
    "\n",
    "    min, max = find_corner(accumulated_heat_map, threshod)\n",
    "    min[0], min[1] = min[1], min[0]\n",
    "    max[0], max[1] = max[1], max[0]\n",
    "    print(accumulated_heat_map.shape)\n",
    "    print('corner',min, max)\n",
    "\n",
    "    #generate heatmap\n",
    "    heatmap = cv2.applyColorMap(resized_heat_map, cv2.COLORMAP_JET)\n",
    "    heatmap1 = cv2.applyColorMap(accumulated_heat_map, cv2.COLORMAP_JET)\n",
    "    out_heatmap = heatmap * 0.3 + img * 0.5\n",
    "    out_heatmap1 = heatmap1 * 0.3 + img * 0.5\n",
    "    out_heatmap[max_x, max_y, :] = [0,255,0]\n",
    "    cv2.imwrite(os.path.join('tmp', classes[idx[0]]) + '_heatmap_ori.jpg', out_heatmap)\n",
    "    cv2.imwrite(os.path.join('tmp', classes[idx[0]]) + '_heatmap_central.jpg', out_heatmap1)\n",
    "    cv2.imwrite(os.path.join('tmp', classes[idx[0]]) + '_heatmap_only_ori.jpg', resized_heat_map)\n",
    "    cv2.imwrite(os.path.join('tmp', classes[idx[0]]) + '_heatmap_only_central.jpg', accumulated_heat_map)\n",
    "    #this code is for find the central code, but don't used\n",
    "    #central_point = cv2.circle(out_heatmap1, (max_x, max_y), 5, (255,0,0))\n",
    "    #cv2.imwrite(os.path.join('tmp', classes[idx[0]]) + '_heatmap_center_point.jpg', central_point)\n",
    "\n",
    "    #generate predict bbox\n",
    "    bbox1 = np.zeros((height, width, 3))\n",
    "    print('bbox1',bbox1.shape)\n",
    "    cv2.rectangle(bbox1, (min[0], min[1]), (max[0], max[1]), (255,255,255))\n",
    "    cv2.putText(bbox1, '{:.3f}, {}'.format(probs[0], classes[idx[0]]), (min[0], min[1]+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 0)\n",
    "\n",
    "    #generate ground truth bbox\n",
    "    anno = parse_rec(annotation_path)\n",
    "    print('annotation', anno)\n",
    "    bbox2 = np.zeros((height, width, 3))\n",
    "\n",
    "    #save image\n",
    "    for item in anno:\n",
    "        cv2.rectangle(bbox2, (item['bbox'][0], item['bbox'][1]), (item['bbox'][2], item['bbox'][3]), (0, 0, 255))\n",
    "    result = bbox1 + img + bbox2\n",
    "    #cv2.imwrite('CAM.jpg', result)\n",
    "    name = os.path.join('tmp', classes[idx[0]]) + '.jpg'\n",
    "    cv2.imwrite(name, result)\n",
    "\n",
    "    #save croped image\n",
    "    croped = img[min[1]:max[1], min[0]:max[0]]\n",
    "    new_x, new_y = max_x-min[1], max_y-min[0]\n",
    "    print(new_x, new_y)\n",
    "    cv2.imwrite(os.path.join('tmp', classes[idx[0]]) + '_croped.jpg', croped)\n",
    "\n",
    "    bbox_loc, kp = surf(croped)\n",
    "    bbox3 = np.zeros((height, width, 3))\n",
    "    left_top = [bbox_loc[0] + min[0], bbox_loc[1] + min[1]]\n",
    "    right_bottom = [bbox_loc[2] + min[0], bbox_loc[3] + min[1]]\n",
    "    cv2.rectangle(bbox3, (left_top[0], left_top[1]), (right_bottom[0], right_bottom[1]), (255,255,255))\n",
    "    #cv2.rectangle(bbox3, (bbox_loc[0], bbox_loc[1]), (bbox_loc[2], bbox_loc[3]), (255,255,255))\n",
    "    cv2.putText(bbox3, '{:.3f}, {}'.format(probs[0], classes[idx[0]]), (left_top[0], left_top[1]+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 0)\n",
    "\n",
    "    surf_output = img + bbox2 + bbox3\n",
    "    cv2.imwrite(os.path.join('tmp', classes[idx[0]]) + 'surf_bbox.jpg', surf_output)\n",
    "    img1 = img.copy()\n",
    "\n",
    "    cv2.drawKeypoints(croped, kp, croped)\n",
    "    print(img1.shape)\n",
    "    print(croped.shape)\n",
    "    print(max[1] - max[1], min[0]-max[0])\n",
    "    img1[min[1]:max[1], min[0]:max[0] ,:] = croped\n",
    "    cv2.imwrite(os.path.join('tmp', classes[idx[0]]) + 'surf_.jpg', img1)\n",
    "\n",
    "    print('end')\n",
    "        \n",
    "    return croped, [max_x, max_y], resized_heat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_shape (312, 352, 3)\n",
      "0.948 -> horse\n",
      "index of class 12\n",
      "size of CAMs 1 224 224\n",
      "(312, 352)\n",
      "(312, 352)\n",
      "corner [23, 15] [299, 278]\n",
      "bbox1 (312, 352, 3)\n",
      "annotation [{'name': 'horse', 'pose': 'Left', 'truncated': 0, 'difficult': 0, 'bbox': [25, 28, 323, 295]}]\n",
      "147 127\n",
      "(263, 276, 3)\n",
      "469\n",
      "11 11 264 251\n",
      "(312, 352, 3)\n",
      "(263, 276, 3)\n",
      "0 -276\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    croped, central_point, heat_map = objectdetection(image_path)\n",
    "    #surf(croped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
